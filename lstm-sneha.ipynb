{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30732,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.autograd as autograd\nimport torch.nn as nn\nimport torch.optim as optim\n\ntorch.manual_seed(1)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T10:35:31.644207Z","iopub.execute_input":"2024-06-07T10:35:31.644719Z","iopub.status.idle":"2024-06-07T10:35:31.653161Z","shell.execute_reply.started":"2024-06-07T10:35:31.644686Z","shell.execute_reply":"2024-06-07T10:35:31.651858Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7df773ff7130>"},"metadata":{}}]},{"cell_type":"code","source":"# Helper functions to make the code more readable.\ndef argmax(vec):\n    # return the argmax as a python int\n    _, idx = torch.max(vec, 1)\n    return idx.item()\n\n\ndef prepare_sequence(seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    return torch.tensor(idxs, dtype=torch.long)\n\n\n# Compute log sum exp in a numerically stable way for the forward algorithm\ndef log_sum_exp(vec):\n    max_score = vec[0, argmax(vec)]\n    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n    return max_score + \\\n        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n\nclass BiLSTM_CRF(nn.Module):\n\n    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n        super(BiLSTM_CRF, self).__init__()\n        self.embedding_dim = embedding_dim\n        self.hidden_dim = hidden_dim\n        self.vocab_size = vocab_size\n        self.tag_to_ix = tag_to_ix\n        self.tagset_size = len(tag_to_ix)\n\n        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n                            num_layers=1, bidirectional=True)\n\n        # Maps the output of the LSTM into tag space.\n        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n\n        # Matrix of transition parameters.  Entry i,j is the score of\n        # transitioning *to* i *from* j.\n        self.transitions = nn.Parameter(\n            torch.randn(self.tagset_size, self.tagset_size))\n\n        # These two statements enforce the constraint that we never transfer\n        # to the start tag and we never transfer from the stop tag\n        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        return (torch.randn(2, 1, self.hidden_dim // 2),\n                torch.randn(2, 1, self.hidden_dim // 2))\n\n    def _forward_alg(self, feats):\n        # Do the forward algorithm to compute the partition function\n        init_alphas = torch.full((1, self.tagset_size), -10000.)\n        # START_TAG has all of the score.\n        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n\n        # Wrap in a variable so that we will get automatic backprop\n        forward_var = init_alphas\n\n        # Iterate through the sentence\n        for feat in feats:\n            alphas_t = []  # The forward tensors at this timestep\n            for next_tag in range(self.tagset_size):\n                # broadcast the emission score: it is the same regardless of\n                # the previous tag\n                emit_score = feat[next_tag].view(\n                    1, -1).expand(1, self.tagset_size)\n                # the ith entry of trans_score is the score of transitioning to\n                # next_tag from i\n                trans_score = self.transitions[next_tag].view(1, -1)\n                # The ith entry of next_tag_var is the value for the\n                # edge (i -> next_tag) before we do log-sum-exp\n                next_tag_var = forward_var + trans_score + emit_score\n                # The forward variable for this tag is log-sum-exp of all the\n                # scores.\n                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n            forward_var = torch.cat(alphas_t).view(1, -1)\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        alpha = log_sum_exp(terminal_var)\n        return alpha\n\n    def _get_lstm_features(self, sentence):\n        self.hidden = self.init_hidden()\n        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n        lstm_feats = self.hidden2tag(lstm_out)\n        return lstm_feats\n\n    def _score_sentence(self, feats, tags):\n        # Gives the score of a provided tag sequence\n        score = torch.zeros(1)\n        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n        for i, feat in enumerate(feats):\n            score = score + \\\n                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n        return score\n\n    def _viterbi_decode(self, feats):\n        backpointers = []\n\n        # Initialize the viterbi variables in log space\n        init_vvars = torch.full((1, self.tagset_size), -10000.)\n        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n\n        # forward_var at step i holds the viterbi variables for step i-1\n        forward_var = init_vvars\n        for feat in feats:\n            bptrs_t = []  # holds the backpointers for this step\n            viterbivars_t = []  # holds the viterbi variables for this step\n\n            for next_tag in range(self.tagset_size):\n                # next_tag_var[i] holds the viterbi variable for tag i at the\n                # previous step, plus the score of transitioning\n                # from tag i to next_tag.\n                # We don't include the emission scores here because the max\n                # does not depend on them (we add them in below)\n                next_tag_var = forward_var + self.transitions[next_tag]\n                best_tag_id = argmax(next_tag_var)\n                bptrs_t.append(best_tag_id)\n                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n            # Now add in the emission scores, and assign forward_var to the set\n            # of viterbi variables we just computed\n            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n            backpointers.append(bptrs_t)\n\n        # Transition to STOP_TAG\n        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n        best_tag_id = argmax(terminal_var)\n        path_score = terminal_var[0][best_tag_id]\n\n        # Follow the back pointers to decode the best path.\n        best_path = [best_tag_id]\n        for bptrs_t in reversed(backpointers):\n            best_tag_id = bptrs_t[best_tag_id]\n            best_path.append(best_tag_id)\n        # Pop off the start tag (we dont want to return that to the caller)\n        start = best_path.pop()\n        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n        best_path.reverse()\n        return path_score, best_path\n\n    def neg_log_likelihood(self, sentence, tags):\n        feats = self._get_lstm_features(sentence)\n        forward_score = self._forward_alg(feats)\n        gold_score = self._score_sentence(feats, tags)\n        return forward_score - gold_score\n\n    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n        # Get the emission scores from the BiLSTM\n        lstm_feats = self._get_lstm_features(sentence)\n\n        # Find the best path, given the features.\n        score, tag_seq = self._viterbi_decode(lstm_feats)\n        return score, tag_seq\n","metadata":{"execution":{"iopub.status.busy":"2024-06-07T10:35:54.413913Z","iopub.execute_input":"2024-06-07T10:35:54.414317Z","iopub.status.idle":"2024-06-07T10:35:54.439488Z","shell.execute_reply.started":"2024-06-07T10:35:54.414260Z","shell.execute_reply":"2024-06-07T10:35:54.438206Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"START_TAG = \"<START>\"\nSTOP_TAG = \"<STOP>\"\nEMBEDDING_DIM = 5\nHIDDEN_DIM = 4\n\n# Make up some training data\ntraining_data = [(\n    \"the wall street journal reported today that apple corporation made money\".split(),\n    \"B I I I O O O B I O O\".split()\n), (\n    \"georgia tech is a university in georgia\".split(),\n    \"B I O O O O B\".split()\n)]\n\nword_to_ix = {}\nfor sentence, tags in training_data:\n    for word in sentence:\n        if word not in word_to_ix:\n            word_to_ix[word] = len(word_to_ix)\n\ntag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}","metadata":{"execution":{"iopub.status.busy":"2024-06-07T10:36:44.418886Z","iopub.execute_input":"2024-06-07T10:36:44.419934Z","iopub.status.idle":"2024-06-07T10:36:44.426276Z","shell.execute_reply.started":"2024-06-07T10:36:44.419894Z","shell.execute_reply":"2024-06-07T10:36:44.425208Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\noptimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T10:37:08.318742Z","iopub.execute_input":"2024-06-07T10:37:08.319136Z","iopub.status.idle":"2024-06-07T10:37:09.863840Z","shell.execute_reply.started":"2024-06-07T10:37:08.319105Z","shell.execute_reply":"2024-06-07T10:37:09.862735Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n    print(model(precheck_sent))","metadata":{"execution":{"iopub.status.busy":"2024-06-07T10:37:30.978981Z","iopub.execute_input":"2024-06-07T10:37:30.979472Z","iopub.status.idle":"2024-06-07T10:37:31.125908Z","shell.execute_reply.started":"2024-06-07T10:37:30.979441Z","shell.execute_reply":"2024-06-07T10:37:31.124844Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"(tensor(2.6907), [1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 1])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Make sure prepare_sequence from earlier in the LSTM section is loaded\nfor epoch in range(\n        300):  # again, normally you would NOT do 300 epochs, it is toy data\n    for sentence, tags in training_data:\n        # Step 1. Remember that Pytorch accumulates gradients.\n        # We need to clear them out before each instance\n        model.zero_grad()\n\n        # Step 2. Get our inputs ready for the network, that is,\n        # turn them into Tensors of word indices.\n        sentence_in = prepare_sequence(sentence, word_to_ix)\n        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n\n        # Step 3. Run our forward pass.\n        loss = model.neg_log_likelihood(sentence_in, targets)\n\n        # Step 4. Compute the loss, gradients, and update the parameters by\n        # calling optimizer.step()\n        loss.backward()\n        optimizer.step()","metadata":{"execution":{"iopub.status.busy":"2024-06-07T10:37:56.179392Z","iopub.execute_input":"2024-06-07T10:37:56.179967Z","iopub.status.idle":"2024-06-07T10:38:04.653171Z","shell.execute_reply.started":"2024-06-07T10:37:56.179922Z","shell.execute_reply":"2024-06-07T10:38:04.652148Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n    print(model(precheck_sent))\n    print(precheck_tags)","metadata":{"execution":{"iopub.status.busy":"2024-06-07T10:38:32.959687Z","iopub.execute_input":"2024-06-07T10:38:32.960084Z","iopub.status.idle":"2024-06-07T10:38:32.970155Z","shell.execute_reply.started":"2024-06-07T10:38:32.960052Z","shell.execute_reply":"2024-06-07T10:38:32.968999Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"(tensor(20.4906), [0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])\ntensor([0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])\n","output_type":"stream"}]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}